{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 2 Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priot to PyTorch 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch \n",
    "# import torchvision\n",
    "\n",
    "# model = torchvision.models.resnet50()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "\n",
    "model = torchvision.models.resnet50()\n",
    "# not support in windows\n",
    "# compliled_model = torch.compile(model)\n",
    "\n",
    "# training code\n",
    "# test code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check PyTorch version\n",
    "pt_version = torch.__version__\n",
    "print(f\"[INFO] Current PyTorch version: {pt_version} (should be 2.x+)\")\n",
    "\n",
    "# Install PyTorch 2.0 if necessary\n",
    "if pt_version.split(\".\")[0] == \"1\": # Check if PyTorch version begins with 1 \n",
    "    !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "    print(\"[INFO] PyTorch 2.x installed, if you're on Google Colab, you may need to restart your runtime.\\\n",
    "          Though as of April 2023, Google Colab comes with PyTorch 2.0 pre-installed.\")\n",
    "    import torch\n",
    "    pt_version = torch.__version__\n",
    "    print(f\"[INFO] Current PyTorch version: {pt_version} (should be 2.x+)\")\n",
    "else:\n",
    "    print(\"[INFO] PyTorch 2.x installed, you'll be able to use the new features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we're using a NVIDIA GPU\n",
    "if torch.cuda.is_available():\n",
    "  gpu_info = !nvidia-smi\n",
    "  gpu_info = '\\n'.join(gpu_info)\n",
    "  if gpu_info.find(\"failed\") >= 0:\n",
    "    print(\"Not connected to a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")\n",
    "\n",
    "  # Get GPU name\n",
    "  gpu_name = !nvidia-smi --query-gpu=gpu_name --format=csv\n",
    "  gpu_name = gpu_name[1]\n",
    "  GPU_NAME = gpu_name.replace(\" \", \"_\") # remove underscores for easier saving\n",
    "  print(f'GPU name: {GPU_NAME}')\n",
    "\n",
    "  # Get GPU capability score\n",
    "  GPU_SCORE = torch.cuda.get_device_capability()\n",
    "  print(f\"GPU capability score: {GPU_SCORE}\")\n",
    "  if GPU_SCORE >= (8, 0):\n",
    "    print(f\"GPU score higher than or equal to (8, 0), PyTorch 2.x speedup features available.\")\n",
    "  else:\n",
    "    print(f\"GPU score lower than (8, 0), PyTorch 2.x speedup features will be limited (PyTorch 2.x speedups happen most on newer GPUs).\")\n",
    "  \n",
    "  # Print GPU info\n",
    "  print(f\"GPU information:\\n{gpu_info}\")\n",
    "\n",
    "else:\n",
    "  print(\"PyTorch couldn't find a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Global set devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Set the device with context manager (requires PyTorch 2.x+)\n",
    "with torch.device(device):\n",
    "    # All tensors created in this block will be on device\n",
    "    layer = torch.nn.Linear(20, 30)\n",
    "    print(f\"Layer weights are on device: {layer.weight.device}\")\n",
    "    print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Set the device globally\n",
    "torch.set_default_device(device)\n",
    "\n",
    "# All tensors created will be on the global device by default\n",
    "layer = torch.nn.Linear(20, 30)\n",
    "print(f\"Layer weights are on device: {layer.weight.device}\")\n",
    "print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "# Set the device globally\n",
    "torch.set_default_device(\"cpu\")\n",
    "\n",
    "# All tensors created will be on \"cpu\"\n",
    "layer = torch.nn.Linear(20, 30)\n",
    "print(f\"Layer weights are on device: {layer.weight.device}\")\n",
    "print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"TorchVision version: {torchvision.__version__}\")\n",
    "\n",
    "# Set the target device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 model and transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model weights and transforms\n",
    "model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2 # <- use the latest weights (could also use .DEFAULT)\n",
    "transforms = model_weights.transforms()\n",
    "\n",
    "# Setup model\n",
    "model = torchvision.models.resnet50(weights=model_weights)\n",
    "\n",
    "# Count the number of parameters in the model \n",
    "total_params = sum(\n",
    "    param.numel() for param in model.parameters() # <- all params\n",
    "\t# param.numel() for param in model.parameters() if param.requires_grad # <- only trainable params\n",
    ")\n",
    "\n",
    "print(f\"Total parameters of model: {total_params} (the more parameters, the more GPU memory the model will use, the more *relative* of a speedup you'll get)\")\n",
    "print(f\"Model transforms:\\n{transforms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes=10):\n",
    "  \"\"\"\n",
    "  Creates a ResNet50 model with the latest weights and transforms via torchvision.\n",
    "  \"\"\"\n",
    "  model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2\n",
    "  transforms = model_weights.transforms()\n",
    "  model = torchvision.models.resnet50(weights=model_weights)\n",
    "  \n",
    "  # Adjust the number of output features in model to match the number of classes in the dataset\n",
    "  model.fc = torch.nn.Linear(in_features=2048, \n",
    "                             out_features=num_classes)\n",
    "  return model, transforms\n",
    "\n",
    "model, transforms = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Speedups are most noticeable when a large portion of the GPU is being used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Checking the memory limits of our GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available GPU memory and total GPU memory \n",
    "total_free_gpu_memory, total_gpu_memory = torch.cuda.mem_get_info()\n",
    "print(f\"Total free GPU memory: {round(total_free_gpu_memory * 1e-9, 3)} GB\")\n",
    "print(f\"Total GPU memory: {round(total_gpu_memory * 1e-9, 3)} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size depending on amount of GPU memory\n",
    "total_free_gpu_memory_gb = round(total_free_gpu_memory * 1e-9, 3)\n",
    "if total_free_gpu_memory_gb >= 16:\n",
    "  BATCH_SIZE = 128 # Note: you could experiment with higher values here if you like.\n",
    "  IMAGE_SIZE = 224\n",
    "  print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE} and image size {IMAGE_SIZE}\")\n",
    "else:\n",
    "  BATCH_SIZE = 32\n",
    "  IMAGE_SIZE = 128\n",
    "  print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE} and image size {IMAGE_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms.crop_size = IMAGE_SIZE\n",
    "transforms.resize_size = IMAGE_SIZE \n",
    "print(f\"Updated data transforms:\\n{transforms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 More potential speedups with TF32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPU_SCORE >= (8, 0):\n",
    "  print(f\"[INFO] Using GPU with score: {GPU_SCORE}, enabling TensorFloat32 (TF32) computing (faster on new GPUs)\")\n",
    "  torch.backends.cuda.matmul.allow_tf32 = True\n",
    "else:\n",
    "  print(f\"[INFO] Using GPU with score: {GPU_SCORE}, TensorFloat32 (TF32) not available, to use it you need a GPU with score >= (8, 0)\")\n",
    "  torch.backends.cuda.matmul.allow_tf32 = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
